# -*- coding: utf-8 -*-
"""model_NLP (2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jkAvvtohoWwaLr1RN7LySkKGGSEXaNxQ
"""

!pip install python-bidi

# Importing necessary libraries

import re  # Regular expression library for string manipulation

# For data handling
import pandas as pd  # Data manipulation library
import numpy as np  # Numerical computing library
import seaborn as sns

# For machine learning and data preprocessing
from sklearn.model_selection import train_test_split  # For splitting data into training and testing sets
from sklearn.preprocessing import LabelEncoder  # For encoding categorical variables
from sklearn.utils.class_weight import compute_class_weight  # For computing class weights
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix  # For evaluating model performance
from sklearn.feature_extraction.text import CountVectorizer  # For converting text data into numerical features
from imblearn.over_sampling import RandomOverSampler  # For handling imbalanced data

# For deep learning frameworks
import os  # For interacting with the operating system
import random  # For generating random numbers
import numpy as np  # Numerical computing library
import tensorflow as tf  # TensorFlow library for deep learning
import pickle

# PyTorch related imports
import torch  # PyTorch library for deep learning
from torch import nn  # Neural network module
from torch.optim import AdamW  # Optimizer for training neural networks
from torch.utils.data import Dataset, DataLoader  # For handling datasets and data loading
from torch.optim.lr_scheduler import ReduceLROnPlateau  # For adjusting learning rate during training

# Transformers from Hugging Face
from transformers import BertModel, BertTokenizerFast, BertForSequenceClassification,AutoTokenizer, AutoModelForSequenceClassification, pipeline  # For using BERT model and tokenizer

# Visualizations
from nltk.util import ngrams  # For generating n-grams from text data
from wordcloud import WordCloud  # For creating word clouds
from collections import Counter  # For counting occurrences of elements
import matplotlib.pyplot as plt  # Plotting library
from bidi.algorithm import get_display  # For handling bidirectional text display

def set_seed(seed=42):
    """Set seed for reproducibility."""
    random.seed(seed)  # Set seed for Python's random module
    np.random.seed(seed)  # Set seed for NumPy
    os.environ['PYTHONHASHSEED'] = str(seed)  # Set Python hash seed
    torch.manual_seed(seed)  # Set seed for PyTorch
    if torch.cuda.is_available():  # Check if CUDA (GPU) is available
        torch.cuda.manual_seed(seed)  # Set seed for CUDA
        torch.cuda.manual_seed_all(seed)  # Set seed for all CUDA devices (if multiple GPUs are available)
    tf.random.set_seed(seed)  # Set seed for TensorFlow

set_seed(42)

# Text preprocessing functions

def remove_html_tags(text):
    """Remove HTML tags from text."""
    return re.sub(r'<[^>]+>', '', text)

def remove_emojis_special_chars(text):
    """Remove emojis and special characters, keeping only Hebrew letters, numbers, and basic punctuation."""
    return re.sub(r'[^\u0590-\u05fe0-9\s.,!?]', '', text)

def remove_diacritics(text):
    """Remove Hebrew diacritics."""
    return re.sub(r'[\u0591-\u05C7]', '', text)

def remove_numbers(text):
    """Remove numbers from text."""
    return re.sub(r'\d+', '', text)

def text_no_punctuation(text):
    """Remove punctuation from text, keeping only word characters and whitespace."""
    return re.sub(r'[^\w\s]', '', text)

def normalize_hebrew_text(text):
    """Normalize repeated characters in text."""
    pattern = re.compile(r"(.)\1{1,}", re.DOTALL)  # Compile regex pattern to match repeated characters
    return pattern.sub(r"\1", text)  # Replace repeated characters with a single occurrence

# Reading the CSV file into a pandas DataFrame
df = pd.read_csv('/content/alldatatagged (2).csv')

# List of text preprocessing functions to be applied to the 'transcriptConsumer' column of the DataFrame

preprocessing_functions = [
    remove_html_tags,          # Function to remove HTML tags from text
    remove_emojis_special_chars,  # Function to remove emojis and special characters, keeping only Hebrew letters, numbers, and basic punctuation
    remove_diacritics,         # Function to remove Hebrew diacritics from text
    remove_numbers,            # Function to remove numbers from text
    text_no_punctuation,       # Function to remove punctuation from text, keeping only word characters and whitespace
    normalize_hebrew_text      # Function to normalize repeated characters in Hebrew text
]

# Applying each preprocessing function to the 'transcriptConsumer' column of the DataFrame
for func in preprocessing_functions:
    df['transcriptConsumer'] = df['transcriptConsumer'].apply(func)

# Drop rows where the 'classification' column has missing values (NaN)
df = df.dropna(subset=['classification '])

# Filtering out rows where the 'classification' column value is 'unclassified'
mask = df['classification '] != 'unclassified'

# Apply the boolean mask to filter rows and update the DataFrame
df = df[mask]

# Extracting unique values from the 'classification' column
unique_classifications = df['classification '].unique()

# Printing the unique values
print(unique_classifications)

# Define the conditions and corresponding categories for creating a new classification column

conditions = [
    # Condition 1: 'urgency' is 1 and 'classification' is 'emotional support'
    (df['urgency'] == 1) & (df['classification '] == 'emotional support'),

    # Condition 2: 'classification' is 'emergency'
    (df['classification '] == 'emergency'),

    # Condition 3: 'urgency' is 1 and 'classification' is 'legal aid'
    (df['urgency'] == 1) & (df['classification '] == 'legal aid'),

    # Condition 4: 'urgency' is 0 and 'classification' is 'emotional support'
    (df['urgency'] == 0) & (df['classification '] == 'emotional support'),

    # Condition 5: 'urgency' is 0 and 'classification' is 'legal aid'
    (df['urgency'] == 0) & (df['classification '] == 'legal aid')
]

# Define corresponding categories for each condition
categories = [
    'emotional support urgent',     # Category for Condition 1
    'emergency',                    # Category for Condition 2
    'legal aid urgent',             # Category for Condition 3
    'emotional support non urgent', # Category for Condition 4
    'legal aid non urgent'          # Category for Condition 5
]

# Using np.select() to create a new column 'new_classification' based on the conditions and categories defined above
df['new_classification'] = np.select(conditions, categories, default='unknown')

# Define columns to check for zero values and the condition for updating the 'new_classification' column

columns_to_check = ['despair', 'loneliness', 'emotional overflow', 'self blame', 'anxiety', 'distrust / confusion', 'urgency']

# Condition for rows where all specified columns have zero values and 'new_classification' is 'emergency'
condition = (df[columns_to_check] == 0).all(axis=1) & (df['new_classification'] == 'emergency')

# Update 'new_classification' column to 'unknown' for rows meeting the condition
df.loc[condition, 'new_classification'] = 'unknown'

# Filter out rows where 'new_classification' is 'unknown'
df = df[df['new_classification'] != 'unknown']

# Define columns to check for zero values and the condition for updating the 'new_classification' column

columns_to_check = ['despair', 'loneliness', 'emotional overflow', 'self blame', 'anxiety', 'distrust / confusion']

# Condition for rows where all specified columns have zero values, 'new_classification' is 'emergency', and 'urgency' is 1
condition = ((df[columns_to_check] == 0).all(axis=1)) & (df['new_classification'] == 'emergency') & (df['urgency'] == 1)

# Update 'new_classification' column to 'unknown' for rows meeting the condition
df.loc[condition, 'new_classification'] = 'unknown'

# Filter out rows where 'new_classification' is 'unknown'
df = df[df['new_classification'] != 'unknown']

# Define columns to check for all values being 1 and the condition for updating the 'new_classification' and 'urgency' columns

columns_to_check = ['despair', 'loneliness', 'emotional overflow', 'self blame', 'anxiety', 'distrust / confusion']

# Condition for rows where all specified columns have value 1
condition = df[columns_to_check].eq(1).all(axis=1)

# Update 'new_classification' column to 'emergency' and 'urgency' column to 1 for rows meeting the condition
df.loc[condition, 'new_classification'] = 'emergency'
df.loc[condition, 'urgency'] = 1

# Filter out rows where the length of the 'transcriptConsumer' column is less than 20 characters
condition = df['transcriptConsumer'].apply(len) >= 20

# Update the DataFrame by keeping only the rows that satisfy the condition
df = df[condition]

# Check for duplicate rows based on the 'transcriptConsumer' column
duplicates = df.duplicated(subset='transcriptConsumer', keep=False)

# Print duplicate rows based on 'transcriptConsumer'
print("Duplicate rows based on 'transcriptConsumer':")
print(df[duplicates])

#drop duplicates rows
df = df.drop_duplicates(subset='transcriptConsumer', keep='first')

# Check for duplicate rows based on the 'transcriptConsumer' column
duplicates = df.duplicated(subset='transcriptConsumer', keep=False)

# Print duplicate rows based on 'transcriptConsumer'
print("Duplicate rows based on 'transcriptConsumer':")
print(df[duplicates])

legalaidurgent = df[df['new_classification'] ==  'legal aid urgent']
legalaidurgent.iloc[1]['transcriptConsumer']
legalaidurgent.to_csv('legalaidurgent.csv', encoding = 'utf-8-sig')

"""# visuals"""

# Counting the frequency of words in the 'transcriptConsumer' column
word_freq = Counter(word for text in df['transcriptConsumer'] for word in text.split())
most_common_words = word_freq.most_common(20)
word_freq_df = pd.DataFrame(most_common_words, columns=['Word', 'Frequency'])
word_freq_df['Word'] = word_freq_df['Word'].apply(get_display)

# Plotting
plt.figure(figsize=(10, 6))
plt.barh(word_freq_df['Word'], word_freq_df['Frequency'], color='skyblue')
plt.title('Top 20 Most Frequent Words')
plt.xlabel('Frequency')
plt.gca().invert_yaxis()  # This makes the y-axis start from top to bottom
plt.show()

def extract_ngrams(data, num):
    words = [word.lower() for word in data.split()]  # lowercasing and splitting words
    n_grams = ngrams(words, num)
    return [' '.join(grams) for grams in n_grams]

# Extracting 2-grams from all rows in the 'transcriptConsumer' column
all_2grams = [ngram for text in df['transcriptConsumer'].dropna() for ngram in extract_ngrams(text, 2)]
ngram_freq = Counter(all_2grams).most_common(20)

# Extracting 2-grams and their frequencies for plotting
ngram_labels = [get_display(n[0]) for n in ngram_freq]
frequencies = [n[1] for n in ngram_freq]

# Creating the plot
plt.figure(figsize=(12, 10))  # Set the figure size to give ample space for 50 items
plt.barh(ngram_labels, frequencies, color='skyblue')  # Create a horizontal bar plot
plt.xlabel('Frequency')  # Label for the x-axis
plt.ylabel('2-grams')  # Label for the y-axis
plt.title('Top 20 Most Common 2-grams')  # Title of the plot
plt.gca().invert_yaxis()  # Invert the y-axis to show the highest values at the top
plt.show()

# Extracting 3-grams from all rows in the 'transcriptConsumer' column
all_3grams = [ngram for text in df['transcriptConsumer'].dropna() for ngram in extract_ngrams(text, 3)]
ngram_freq = Counter(all_3grams).most_common(20)

# Extracting 3-grams and their frequencies for plotting
ngram_labels = [get_display(n[0]) for n in ngram_freq]
frequencies = [n[1] for n in ngram_freq]

# Creating the plot
plt.figure(figsize=(12, 10))  # Set the figure size to give ample space for 50 items
plt.barh(ngram_labels, frequencies, color='skyblue')  # Create a horizontal bar plot
plt.xlabel('Frequency')  # Label for the x-axis
plt.ylabel('3-grams')  # Label for the y-axis
plt.title('Top 20 Most Common 3-grams')  # Title of the plot
plt.gca().invert_yaxis()  # Invert the y-axis to show the highest values at the top
plt.show()

# Extracting 5-grams from all rows in the 'transcriptConsumer' column
all_5grams = [ngram for text in df['transcriptConsumer'].dropna() for ngram in extract_ngrams(text, 5)]
ngram_freq = Counter(all_5grams).most_common(20)

# Extracting 5-grams and their frequencies for plotting
ngram_labels = [get_display(n[0]) for n in ngram_freq]
frequencies = [n[1] for n in ngram_freq]

# Creating the plot
plt.figure(figsize=(12, 10))  # Set the figure size to give ample space for 50 items
plt.barh(ngram_labels, frequencies, color='skyblue')  # Create a horizontal bar plot
plt.xlabel('Frequency')  # Label for the x-axis
plt.ylabel('5-grams')  # Label for the y-axis
plt.title('Top 20 Most Common 5-grams')  # Title of the plot
plt.gca().invert_yaxis()  # Invert the y-axis to show the highest values at the top
plt.show()

# Text to drop as its the same conversation, only keeping one
text_to_search = 'שחבקתי ונשקתי את אמא'
mask = df['transcriptConsumer'].str.contains(text_to_search, na=False)
rows_to_drop = mask & (mask.cumsum() > 1)
df = df[~rows_to_drop]

# Text to drop as its the same conversation, only keeping one
text_to_search = 'לראות פלאשבקים מהתיכון'
mask = df['transcriptConsumer'].str.contains(text_to_search, na=False)
rows_to_drop = mask & (mask.cumsum() > 1)
df = df[~rows_to_drop]

# Document length
doc_lengths = df['transcriptConsumer'].apply(lambda text: len(text.split()))

# Plotting
plt.figure(figsize=(8, 6))
plt.hist(doc_lengths, bins=10, color='purple')
plt.title('Document Length Distribution')
plt.xlabel('Document Length')
plt.ylabel('Frequency')
plt.show()

classification_counts = df['new_classification'].value_counts()

# Plotting the counts
plt.figure(figsize=(12, 8))  # Increase the figure size for better visibility
ax = classification_counts.plot(kind='bar', color='skyblue')  # Create a bar plot and store the axis
plt.title('Distribution of Classification Categories')  # Add a title
plt.xlabel('Categories')  # Label the x-axis
plt.ylabel('Frequency')  # Label the y-axis
plt.xticks(rotation=45)  # Rotate category labels for better visibility
plt.grid(True, which='both', linestyle='--', linewidth=0.5)  # Add a grid for easier reference

# Annotate each bar with the count of occurrences
for p in ax.patches:  # Loop through all the bars in the plot
    ax.annotate(str(p.get_height()),  # Get the height of each bar
                (p.get_x() + p.get_width() / 2., p.get_height()),  # Position for the text
                ha='center', va='center', xytext=(0, 10), textcoords='offset points', fontsize=10)

plt.tight_layout()  # Adjust layout to make room for rotated x-axis labels
plt.show()  # Display the plot

"""# Training Proccess"""

# Encode the 'new_classification' column using LabelEncoder to convert categorical labels into numerical values
le = LabelEncoder()
df['category_encoded'] = le.fit_transform(df['new_classification'])
df['category_encoded'].nunique()

# Encode the 'new_classification' column using LabelEncoder to convert categorical labels into numerical values
y_encoded = le.fit_transform(df['new_classification'])

# Drop the label column 'new_classification' from the feature matrix
X = df.drop(['new_classification'], axis=1)

# Apply Random Over Sampling to handle class imbalance
ros = RandomOverSampler(random_state=42)
X_resampled, y_resampled = ros.fit_resample(X, y_encoded)

# Convert resampled feature matrix and encoded labels back to a DataFrame
resampled_df = pd.DataFrame(X_resampled, columns=X.columns)
resampled_df['new_classification'] = le.inverse_transform(y_resampled)

#Use the model without finetuning

# Load the tokenizer and model
tokenizer = AutoTokenizer.from_pretrained('onlplab/alephbert-base')
model = AutoModelForSequenceClassification.from_pretrained('onlplab/alephbert-base', num_labels=len(le.classes_))

# Assume DataFrame 'resampled_df' is loaded and has the correct format
texts = resampled_df['transcriptConsumer'].tolist()

# Create a pipeline for classification with truncation to handle token length limit
classification_pipeline = pipeline(
    'text-classification',
    model=model,
    tokenizer=tokenizer,
    device=0,  # use GPU if available
    truncation=True,
    max_length=512  # Ensure no input exceeds 512 tokens
)

# Perform inference
predictions = classification_pipeline(texts)

# Convert predictions to DataFrame for easier handling
predictions_df = pd.DataFrame(predictions)
predicted_labels = [pred['label'] for pred in predictions]
predicted_scores = [pred['score'] for pred in predictions]

# Convert numerical labels back to original classes
predicted_labels = le.inverse_transform([int(label.split('_')[-1]) for label in predicted_labels])

# Add predictions to DataFrame
resampled_df['predicted_classification'] = predicted_labels
resampled_df['prediction_score'] = predicted_scores

# Evaluation
from sklearn.metrics import accuracy_score, classification_report
print("Accuracy:", accuracy_score(resampled_df['new_classification'], predicted_labels))
print("Classification Report:", classification_report(resampled_df['new_classification'], predicted_labels))

# Display some predictions
print(resampled_df[['new_classification', 'predicted_classification', 'prediction_score']].head())

class EarlyStopping:
    def __init__(self, patience=3, verbose=False, delta=0):
        """
        Args:
            patience (int): How long to wait after last time validation loss improved.
                            Default: 3
            verbose (bool): If True, prints a message for each validation loss improvement.
                            Default: False
            delta (float): Minimum change in the monitored quantity to qualify as an improvement.
                           Default: 0
        """
        self.patience = patience
        self.verbose = verbose
        self.delta = delta
        self.counter = 0
        self.best_loss = None
        self.early_stop = False

    def __call__(self, val_loss):
        if self.best_loss is None:
            self.best_loss = val_loss
        elif val_loss > self.best_loss - self.delta:
            self.counter += 1
            if self.verbose:
                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_loss = val_loss
            self.counter = 0

# Initialize tokenizer from pretrained model
tokenizer = BertTokenizerFast.from_pretrained('onlplab/alephbert-base')

# Label encoding
num_labels = len(le.classes_)

# Define additional features and label
labels_column = 'category_encoded'
text_column = 'transcriptConsumer'

# Split the dataset into train and test sets, with stratification
train_texts, test_texts, train_labels, test_labels = train_test_split(
    resampled_df[text_column], resampled_df[labels_column], test_size=0.2, stratify=resampled_df[labels_column]
)

# Further split the train set into train and validation sets
train_texts, val_texts, train_labels, val_labels = train_test_split(
    train_texts, train_labels, test_size=0.2
)

# Tokenize text data for train, validation, and test sets
train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True, max_length=128, return_tensors="pt")
val_encodings = tokenizer(val_texts.tolist(), truncation=True, padding=True, max_length=128, return_tensors="pt")
test_encodings = tokenizer(test_texts.tolist(), truncation=True, padding=True, max_length=128, return_tensors="pt")

# Compute class weights for imbalance handling
class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)
weights_tensor = torch.tensor(class_weights, dtype=torch.float)

# Define PyTorch dataset for text classification
class TextDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = torch.tensor(labels, dtype=torch.long)  # Ensure labels are converted to tensor

    def __getitem__(self, idx):
        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}
        item['labels'] = self.labels[idx]  # Now self.labels is already a tensor
        return item

    def __len__(self):
        return len(self.labels)

# Prepare datasets and dataloaders for train, validation, and test sets
train_dataset = TextDataset(train_encodings, train_labels.tolist())
val_dataset = TextDataset(val_encodings, val_labels.tolist())
test_dataset = TextDataset(test_encodings, test_labels.tolist())

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)

# Define the text classification model
class BertSimple(nn.Module):
    def __init__(self, num_labels):
        super().__init__()
        self.bert = BertModel.from_pretrained('onlplab/alephbert-base')
        self.dropout = nn.Dropout(0.1)
        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = self.dropout(outputs.pooler_output)
        logits = self.classifier(pooled_output)
        return logits

# Initialize the text classification model
model = BertSimple(num_labels)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

# Using AdamW optimizer with L2 regularization
optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)  # Adjust weight_decay as needed
loss_fn = nn.CrossEntropyLoss(weight=weights_tensor.to(device))

# Scheduler and early stopping
scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)
early_stopping = EarlyStopping(patience=3, verbose=True)

### Training Loop ###

# Initialize lists to store training and validation losses, as well as accuracies
train_losses = []
val_losses = []
train_accuracies = []
val_accuracies = []

# Training loop
for epoch in range(100):  # Example epoch count
    model.train()
    total_loss = 0
    total = 0
    correct = 0
    # Iterate over batches in the training DataLoader
    for batch in train_loader:
        optimizer.zero_grad()
        # Prepare inputs correctly
        inputs = {
            'input_ids': batch['input_ids'].to(device),
            'attention_mask': batch['attention_mask'].to(device)
        }
        labels = batch['labels'].to(device)
        # Pass only the expected inputs
        outputs = model(**inputs)
        loss = loss_fn(outputs, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
    # Calculate and store training loss and accuracy for the epoch
    train_losses.append(total_loss / len(train_loader))
    train_accuracy = 100 * correct / total
    train_accuracies.append(train_accuracy)
    print(f"Epoch {epoch+1}, Train Loss: {train_losses[-1]}, Train Accuracy: {train_accuracy}")

    model.eval()
    val_loss = 0
    correct = 0
    total = 0
    with torch.no_grad():
        # Iterate over batches in the validation DataLoader
        for batch in val_loader:
            inputs = {
                'input_ids': batch['input_ids'].to(device),
                'attention_mask': batch['attention_mask'].to(device)
            }
            labels = batch['labels'].to(device)
            outputs = model(**inputs)
            batch_loss = loss_fn(outputs, labels)
            val_loss += batch_loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    # Calculate and store validation loss and accuracy for the epoch
    val_losses.append(val_loss / len(val_loader))
    val_accuracy = 100 * correct / total
    val_accuracies.append(val_accuracy)
    print(f"Epoch {epoch+1}, Validation Loss: {val_losses[-1]}, Validation Accuracy: {val_accuracy}")
    # Check for early stopping
    early_stopping(val_loss / len(val_loader))
    if early_stopping.early_stop:
        print("Early stopping triggered")
        break

##### Validation #####

# Switch model to evaluation mode
model.eval()

# Initialize lists to store true labels and predictions
true_labels = []
predictions = []
probabilities = []
# Set device for evaluation
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# No gradient needed for evaluation
with torch.no_grad():
    for batch in val_loader:
        # Prepare input data by excluding 'labels' from being sent to the model
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)

        # Forward pass without labels
        logits = model(input_ids=input_ids, attention_mask=attention_mask)  # Model directly returns logits

        # Convert logits to probabilities using softmax
        softmax = torch.nn.Softmax(dim=1)
        probs = softmax(logits).cpu().numpy()

        # Convert logits to predictions
        preds = torch.argmax(logits, dim=1).cpu().numpy()  # Move predictions back to CPU
        labels = batch['labels'].cpu().numpy()  # Extract labels for evaluation

        true_labels.extend(labels)
        predictions.extend(preds)
        probabilities.extend(probs)
# Calculate and print evaluation metrics
print(f"Validation Accuracy: {accuracy_score(true_labels, predictions)}")
print(classification_report(true_labels, predictions, target_names=['emergency', 'emotional support non urgent', 'emotional support urgent', 'legal aid non urgent', 'legal aid urgent']))
print(confusion_matrix(true_labels, predictions))

#### Test ####

# Ensure the model is in evaluation mode
model.eval()

# Variables to gather results
test_loss = 0
correct = 0
total = 0

# We do not need to track gradients here, so we use no_grad to save memory
with torch.no_grad():
    for batch in test_loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        logits = outputs
        loss = loss_fn(logits, labels)

        test_loss += loss.item()
        preds = torch.argmax(logits, dim=1)
        correct += (preds == labels).sum().item()
        total += labels.size(0)

# Calculate average losses and total accuracy
test_loss /= len(test_loader)
accuracy = correct / total * 100

print(f"Test Loss: {test_loss:.4f}, Test Accuracy: {accuracy:.2f}%")

# Gather all predictions and true labels
all_preds = []
all_true = []
with torch.no_grad():
    for batch in test_loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        logits = outputs
        preds = torch.argmax(logits, dim=1).cpu().numpy()
        labels = batch['labels'].cpu().numpy()

        all_preds.extend(preds)
        all_true.extend(labels)

# Print the classification report and confusion matrix
print(classification_report(all_true, all_preds, target_names=le.classes_))
print(confusion_matrix(all_true, all_preds))

"""#Visual

"""

# Plot the training and validation losses over epochs
plt.figure(figsize=(10, 5))
plt.plot(train_losses, label='Training Loss')
plt.plot(val_losses, label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

# Plot the training and validation accuracies over epochs
plt.figure(figsize=(10, 5))
plt.plot(train_accuracies, label='Training Accuracy')
plt.plot(val_accuracies, label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.show()

# Plot the confusion matrix
cm = confusion_matrix(true_labels, predictions)
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt="d", cmap='Blues', xticklabels=['emergency', 'emotional support non urgent', 'emotional support urgent', 'legal aid non urgent', 'legal aid urgent'], yticklabels=['emergency', 'emotional support non urgent', 'emotional support urgent', 'legal aid non urgent', 'legal aid urgent'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import label_binarize
import matplotlib.pyplot as plt

# Binarize the labels for multi-class ROC analysis
y_bin = label_binarize(true_labels, classes=[0,1,2,3,4])  # Adjust class numbers as needed
n_classes = y_bin.shape[1]

# Compute ROC curve and ROC area for each class
for i in range(n_classes):
    fpr, tpr, _ = roc_curve(y_bin[:, i], [p[i] for p in probabilities])
    roc_auc = auc(fpr, tpr)

    plt.figure()
    plt.plot(fpr, tpr, label=f'ROC curve (area = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'Receiver Operating Characteristic for class {i}')
    plt.legend(loc="lower right")
    plt.show()

from sklearn.metrics import precision_recall_curve, average_precision_score

for i in range(n_classes):
    precision, recall, _ = precision_recall_curve(y_bin[:, i], [p[i] for p in probabilities])
    ap = average_precision_score(y_bin[:, i], [p[i] for p in probabilities])

    plt.figure()
    plt.step(recall, precision, where='post', label=f'AP = {ap:.2f}')
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.ylim([0.0, 1.05])
    plt.xlim([0.0, 1.0])
    plt.title(f'Precision-Recall curve for class {i}')
    plt.legend(loc="lower right")
    plt.show()

# Save the model's state dictionary
torch.save(model.state_dict(), "/content/model_state_dict.pth")

# Save the tokenizer
with open("tokenizer.pkl", "wb") as f:
    pickle.dump(tokenizer, f)